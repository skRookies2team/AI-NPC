"""
RAGAS í‰ê°€ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
ì‹¤ì œ ì„¸ì…˜ì´ ì—†ì–´ë„ ìƒ˜í”Œ ë°ì´í„°ë¡œ í‰ê°€ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

import os
import json
import sys
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# RAGASì™€ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
try:
    from ragas import evaluate
    from ragas.metrics import (
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
    )
    from datasets import Dataset
    from langchain_openai import OpenAIEmbeddings, ChatOpenAI
    from langchain_community.vectorstores import PGVector
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.runnables import RunnablePassthrough
    from langchain_core.output_parsers import StrOutputParser
except ImportError as e:
    print(f"âŒ í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
    print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install ragas datasets")
    sys.exit(1)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
POSTGRES_CONNECTION_STRING = os.getenv("POSTGRES_CONNECTION_STRING")

if not OPENAI_API_KEY:
    print("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    sys.exit(1)


def create_sample_evaluation_data():
    """ìƒ˜í”Œ í‰ê°€ ë°ì´í„° ìƒì„±"""
    return [
        {
            "question": "ìºë¦­í„°ì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ground_truth": "ìºë¦­í„°ì˜ ì´ë¦„ì€ ì†Œì„¤ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        },
        {
            "question": "ìºë¦­í„°ì˜ ì„±ê²©ì€ ì–´ë–¤ê°€ìš”?",
            "ground_truth": "ìºë¦­í„°ì˜ ì„±ê²©ì€ ì†Œì„¤ ë‚´ìš©ì— ê¸°ë°˜í•©ë‹ˆë‹¤.",
        },
        {
            "question": "ì£¼ìš” ì‚¬ê±´ì€ ë¬´ì—‡ì´ì—ˆë‚˜ìš”?",
            "ground_truth": "ì£¼ìš” ì‚¬ê±´ì€ ì†Œì„¤ì˜ ë‚´ìš©ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
        },
    ]


def run_evaluation_with_session(session_id: str, character_name: str = "ìºë¦­í„°"):
    """ì‹¤ì œ ì„¸ì…˜ìœ¼ë¡œ í‰ê°€ ì‹¤í–‰"""
    print(f"ğŸ“Š RAGAS í‰ê°€ ì‹œì‘...")
    print(f"ì„¸ì…˜ ID: {session_id}")
    print("-" * 50)
    
    try:
        # PostgreSQL Vector Storeì—ì„œ retriever ë¡œë“œ
        embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
        collection_name = f"session_{session_id}"
        
        vectorstore = PGVector(
            collection_name=collection_name,
            connection_string=POSTGRES_CONNECTION_STRING,
            embedding_function=embeddings,
            use_jsonb=True
        )
        
        retriever = vectorstore.as_retriever(search_kwargs={"k": 6})
        print("âœ… ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("ìƒ˜í”Œ ë°ì´í„°ë¡œ í‰ê°€ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤...")
        return run_sample_evaluation()
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ë²”ìš©ì , ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ê°•ì œ)
    template = f"""
    ë‹¹ì‹ ì€ ì†Œì„¤ ì† ì¸ë¬¼ '{character_name}'ì…ë‹ˆë‹¤.
    
    **ì¤‘ìš” ê·œì¹™:**
    1. ì•„ë˜ [Context]ì— ìˆëŠ” ì†Œì„¤ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
    2. [Context]ì— ì—†ëŠ” ì •ë³´ëŠ” ì ˆëŒ€ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
    3. [Context]ì— ë‹µë³€í•  ìˆ˜ ìˆëŠ” ì •ë³´ê°€ ì—†ìœ¼ë©´ "ì†Œì„¤ ë‚´ìš©ì— ê·¸ëŸ° ì •ë³´ëŠ” ë‚˜ì˜¤ì§€ ì•ŠìŠµë‹ˆë‹¤" ë˜ëŠ” "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤"ë¼ê³  ì†”ì§í•˜ê²Œ ë§í•˜ì„¸ìš”.
    4. ì»¨í…ìŠ¤íŠ¸ ë°–ì˜ ì¼ë°˜ ì§€ì‹ì´ë‚˜ ì¶”ì¸¡ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
    5. ì†Œì„¤ì— ë‚˜ì˜¤ëŠ” ì¸ë¬¼, ì¥ì†Œ, ì‚¬ê±´ì˜ ì´ë¦„ê³¼ í‘œí˜„ì„ ì •í™•íˆ ì‚¬ìš©í•˜ì„¸ìš”.
    
    [Context]:
    {{context}}
    """
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", template),
        ("human", "{question}")
    ])
    
    llm = ChatOpenAI(model="gpt-4o", temperature=0.7, openai_api_key=OPENAI_API_KEY)
    
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)
    
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    # í‰ê°€ ë°ì´í„° ì¤€ë¹„
    eval_data = create_sample_evaluation_data()
    print(f"âœ… í‰ê°€ ë°ì´í„° {len(eval_data)}ê°œ ì¤€ë¹„ ì™„ë£Œ")
    
    # ê° ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€ ìƒì„± ë° ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
    questions = []
    answers = []
    contexts_list = []
    ground_truths_list = []
    
    print("\nğŸ”„ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘...")
    for i, item in enumerate(eval_data):
        question = item["question"]
        questions.append(question)
        
        # Ground truth ì²˜ë¦¬
        if "ground_truths" in item:
            ground_truths_list.append(item["ground_truths"])
        elif "ground_truth" in item:
            ground_truths_list.append([item["ground_truth"]])
        else:
            ground_truths_list.append([])
        
        # RAG ì²´ì¸ìœ¼ë¡œ ë‹µë³€ ìƒì„±
        print(f"  [{i+1}/{len(eval_data)}] ì§ˆë¬¸: {question[:50]}...")
        try:
            answer = rag_chain.invoke(question)
            answers.append(answer)
            
            # ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ (LangChain ìµœì‹  ë²„ì „ì€ invoke ì‚¬ìš©)
            docs = retriever.invoke(question)
            contexts = [doc.page_content for doc in docs[:4]]  # 4ê°œë§Œ ì„ íƒ
            contexts_list.append(contexts)
        except Exception as e:
            print(f"    âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
            answers.append("ë‹µë³€ ìƒì„± ì‹¤íŒ¨")
            contexts_list.append([])
    
    return evaluate_metrics(questions, answers, contexts_list, ground_truths_list, session_id)


def run_sample_evaluation():
    """ìƒ˜í”Œ ë°ì´í„°ë¡œ í‰ê°€ ì‹¤í–‰ (ì‹¤ì œ ì„¸ì…˜ ì—†ì´)"""
    print("ğŸ“Š RAGAS ìƒ˜í”Œ í‰ê°€ ì‹œì‘...")
    print("(ì‹¤ì œ ì„¸ì…˜ì´ ì—†ì–´ ìƒ˜í”Œ ë°ì´í„°ë¡œ í‰ê°€í•©ë‹ˆë‹¤)")
    print("-" * 50)
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    questions = [
        "ìºë¦­í„°ì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ìºë¦­í„°ì˜ ì„±ê²©ì€ ì–´ë–¤ê°€ìš”?",
        "ì£¼ìš” ì‚¬ê±´ì€ ë¬´ì—‡ì´ì—ˆë‚˜ìš”?",
    ]
    
    # ìƒ˜í”Œ ë‹µë³€ (ì‹¤ì œë¡œëŠ” RAG ì²´ì¸ì—ì„œ ìƒì„±ë¨)
    answers = [
        "ìºë¦­í„°ì˜ ì´ë¦„ì€ ì†Œì„¤ ë‚´ìš©ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "ìºë¦­í„°ëŠ” ì†Œì„¤ì— ë¬˜ì‚¬ëœ ì„±ê²©ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.",
        "ì£¼ìš” ì‚¬ê±´ì€ ì†Œì„¤ì˜ ì¤„ê±°ë¦¬ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
    ]
    
    # ìƒ˜í”Œ ì»¨í…ìŠ¤íŠ¸
    contexts_list = [
        ["ì†Œì„¤ì˜ ì²« ë²ˆì§¸ ë¶€ë¶„ì…ë‹ˆë‹¤. ìºë¦­í„°ì— ëŒ€í•œ ì„¤ëª…ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤."],
        ["ì†Œì„¤ì˜ ë‘ ë²ˆì§¸ ë¶€ë¶„ì…ë‹ˆë‹¤. ìºë¦­í„°ì˜ ì„±ê²©ì´ ë¬˜ì‚¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤."],
        ["ì†Œì„¤ì˜ ì„¸ ë²ˆì§¸ ë¶€ë¶„ì…ë‹ˆë‹¤. ì£¼ìš” ì‚¬ê±´ì´ ì „ê°œë©ë‹ˆë‹¤."],
    ]
    
    ground_truths_list = [
        ["ìºë¦­í„°ì˜ ì´ë¦„ì€ ì†Œì„¤ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."],
        ["ìºë¦­í„°ì˜ ì„±ê²©ì€ ì†Œì„¤ ë‚´ìš©ì— ê¸°ë°˜í•©ë‹ˆë‹¤."],
        ["ì£¼ìš” ì‚¬ê±´ì€ ì†Œì„¤ì˜ ë‚´ìš©ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤."],
    ]
    
    print(f"âœ… ìƒ˜í”Œ ë°ì´í„° {len(questions)}ê°œ ì¤€ë¹„ ì™„ë£Œ")
    
    return evaluate_metrics(questions, answers, contexts_list, ground_truths_list, "sample")


def evaluate_metrics(questions, answers, contexts_list, ground_truths_list, session_id):
    """RAGAS ë©”íŠ¸ë¦­ í‰ê°€"""
    print("\nğŸ“ˆ RAGAS ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
    
    try:
        # reference ì»¬ëŸ¼ ìƒì„± (ground_truthsì˜ ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©)
        # context_precisionì€ referenceê°€ í•„ìš”í•¨
        references = []
        for gt_list in ground_truths_list:
            if gt_list and len(gt_list) > 0:
                references.append(gt_list[0])  # ì²« ë²ˆì§¸ ground truthë¥¼ referenceë¡œ ì‚¬ìš©
            else:
                references.append("")  # ë¹ˆ ë¬¸ìì—´
        
        # RAGAS í‰ê°€ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹ ìƒì„±
        evaluation_dataset = Dataset.from_dict({
            "question": questions,
            "answer": answers,
            "contexts": contexts_list,
            "ground_truths": ground_truths_list,
            "reference": references,  # context_precisionì„ ìœ„í•´ ì¶”ê°€
        })
        
        # RAGAS í‰ê°€ ì‹¤í–‰ (embeddings ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬)
        embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
        
        result = evaluate(
            dataset=evaluation_dataset,
            metrics=[
                faithfulness,
                answer_relevancy,  # embeddings í•„ìš”
                context_precision,
                context_recall,
            ],
            embeddings=embeddings,  # RAGASì— embeddings ì „ë‹¬
        )
        
        # ê²°ê³¼ ì¶œë ¥
        print_results(result, len(questions), session_id)
        
        return result
        
    except Exception as e:
        print(f"âŒ í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("\nğŸ’¡ ì°¸ê³ : ì‹¤ì œ í‰ê°€ë¥¼ ìœ„í•´ì„œëŠ”:")
        print("   1. PostgreSQLì— ë²¡í„° ìŠ¤í† ì–´ê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤")
        print("   2. OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤")
        print("   3. ragasì™€ datasets íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤")
        return None


def print_results(result, num_questions, session_id):
    """ê²°ê³¼ ì¶œë ¥"""
    print("\n" + "=" * 60)
    print("ğŸ“Š RAGAS í‰ê°€ ê²°ê³¼")
    print("=" * 60)
    print(f"\ní‰ê°€ëœ ì§ˆë¬¸ ìˆ˜: {num_questions}")
    print(f"\nğŸ“ˆ ë©”íŠ¸ë¦­ ì ìˆ˜:")
    
    # EvaluationResult ê°ì²´ì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°
    def get_metric_value(metric_name):
        try:
            # ì†ì„±ìœ¼ë¡œ ì ‘ê·¼ ì‹œë„
            value = getattr(result, metric_name, None)
            if value is None:
                # ë”•ì…”ë„ˆë¦¬ì²˜ëŸ¼ ì ‘ê·¼ ì‹œë„
                value = result[metric_name] if hasattr(result, '__getitem__') else None
            if value is None:
                return 0.0
            # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° í‰ê· ê°’ ì‚¬ìš©
            if isinstance(value, list):
                return sum(value) / len(value) if value else 0.0
            # ìˆ«ìì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
            return float(value)
        except (TypeError, ValueError, AttributeError, KeyError):
            return 0.0
    
    metrics = {
        "faithfulness": get_metric_value('faithfulness'),
        "answer_relevancy": get_metric_value('answer_relevancy'),
        "context_precision": get_metric_value('context_precision'),
        "context_recall": get_metric_value('context_recall'),
    }
    
    for metric_name, score in metrics.items():
        score_value = float(score) if hasattr(score, '__float__') else 0.0
        bar_length = int(score_value * 40)
        bar = "â–ˆ" * bar_length + "â–‘" * (40 - bar_length)
        
        metric_kr = {
            "faithfulness": "Faithfulness (ì •í™•ì„±)",
            "answer_relevancy": "Answer Relevancy (ë‹µë³€ ê´€ë ¨ì„±)",
            "context_precision": "Context Precision (ì»¨í…ìŠ¤íŠ¸ ì •ë°€ë„)",
            "context_recall": "Context Recall (ì»¨í…ìŠ¤íŠ¸ ì¬í˜„ìœ¨)",
        }.get(metric_name, metric_name)
        
        print(f"  {metric_kr:35s}: {score_value:.4f} [{bar}]")
    
    # ì „ì²´ í‰ê·  ê³„ì‚°
    avg_score = sum(metrics.values()) / len(metrics) if metrics else 0
    avg_bar = "â–ˆ" * int(avg_score * 40) + "â–‘" * (40 - int(avg_score * 40))
    print(f"\n  {'ì „ì²´ í‰ê·  ì ìˆ˜':35s}: {avg_score:.4f} [{avg_bar}]")
    
    # ê²°ê³¼ í•´ì„
    print("\nğŸ“ ê²°ê³¼ í•´ì„:")
    if avg_score >= 0.8:
        print("  âœ… ìš°ìˆ˜í•œ ì„±ëŠ¥ì…ë‹ˆë‹¤!")
    elif avg_score >= 0.6:
        print("  âš ï¸ ë³´í†µ ì„±ëŠ¥ì…ë‹ˆë‹¤. ê°œì„  ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("  âŒ ì„±ëŠ¥ì´ ë‚®ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # ê²°ê³¼ ì €ì¥
    output_file = f"evaluation_results_{session_id}.json"
    results_dict = {
        "session_id": session_id,
        "num_questions": num_questions,
        "metrics": {k: float(v) if hasattr(v, '__float__') else 0.0 for k, v in metrics.items()},
        "average": float(avg_score),
    }
    
    try:
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(results_dict, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ê²°ê³¼ê°€ '{output_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâš ï¸ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="RAGAS í‰ê°€ ì‹¤í–‰")
    parser.add_argument(
        "--session_id",
        type=str,
        help="í‰ê°€í•  ì„¸ì…˜ ID (ì—†ìœ¼ë©´ ìƒ˜í”Œ í‰ê°€ ì‹¤í–‰)"
    )
    parser.add_argument(
        "--character_name",
        type=str,
        default="ìºë¦­í„°",
        help="ìºë¦­í„° ì´ë¦„"
    )
    
    args = parser.parse_args()
    
    if args.session_id:
        run_evaluation_with_session(args.session_id, args.character_name)
    else:
        print("ì„¸ì…˜ IDê°€ ì œê³µë˜ì§€ ì•Šì•„ ìƒ˜í”Œ í‰ê°€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        print("ì‹¤ì œ ì„¸ì…˜ìœ¼ë¡œ í‰ê°€í•˜ë ¤ë©´: python run_evaluation.py --session_id <ì„¸ì…˜ID>\n")
        run_sample_evaluation()

