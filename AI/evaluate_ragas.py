"""
RAGASë¥¼ ì‚¬ìš©í•œ RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš© ë°©ë²•:
1. í‰ê°€ ë°ì´í„°ì…‹ ì¤€ë¹„ (evaluation_dataset.json)
2. python evaluate_ragas.py --session_id <ì„¸ì…˜ID> --dataset evaluation_dataset.json

í‰ê°€ ë©”íŠ¸ë¦­:
- faithfulness: ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•œ ì •í™•ì„± (0-1)
- answer_relevancy: ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ (0-1)
- context_precision: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë°€ë„ (0-1)
- context_recall: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì˜ ì¬í˜„ìœ¨ (0-1)
"""

import os
import json
import argparse
from typing import List, Dict
from dotenv import load_dotenv

# RAGAS ê´€ë ¨ ì„í¬íŠ¸
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from datasets import Dataset

# LangChain ê´€ë ¨ ì„í¬íŠ¸
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import PGVector
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
POSTGRES_CONNECTION_STRING = os.getenv("POSTGRES_CONNECTION_STRING")

if not OPENAI_API_KEY:
    raise ValueError(".env íŒŒì¼ì— OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
if not POSTGRES_CONNECTION_STRING:
    raise ValueError(".env íŒŒì¼ì— POSTGRES_CONNECTION_STRINGì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


def load_evaluation_dataset(file_path: str) -> List[Dict]:
    """
    í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ
    
    ë°ì´í„°ì…‹ í˜•ì‹:
    [
        {
            "question": "ì§ˆë¬¸",
            "ground_truth": "ì •ë‹µ (ì„ íƒì‚¬í•­)",
            "ground_truths": ["ì •ë‹µ1", "ì •ë‹µ2"] (ì„ íƒì‚¬í•­)
        },
        ...
    ]
    """
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data


def create_rag_chain(session_id: str, character_name: str = "ìºë¦­í„°", k: int = 6):
    """
    RAG ì²´ì¸ ìƒì„± (ai_server.pyì™€ ë™ì¼í•œ êµ¬ì¡°)
    """
    # PostgreSQL Vector Storeì—ì„œ retriever ë¡œë“œ
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    collection_name = f"session_{session_id}"
    
    vectorstore = PGVector(
        collection_name=collection_name,
        connection_string=POSTGRES_CONNECTION_STRING,
        embedding_function=embeddings,
        use_jsonb=True
    )
    
    retriever = vectorstore.as_retriever(search_kwargs={"k": k})
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ë²”ìš©ì , ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ê°•ì œ)
    template = f"""
    ë‹¹ì‹ ì€ ì†Œì„¤ ì† ì¸ë¬¼ '{character_name}'ì…ë‹ˆë‹¤.
    
    **ì¤‘ìš” ê·œì¹™:**
    1. ì•„ë˜ [Context]ì— ìˆëŠ” ì†Œì„¤ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
    2. [Context]ì— ì—†ëŠ” ì •ë³´ëŠ” ì ˆëŒ€ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
    3. [Context]ì— ë‹µë³€í•  ìˆ˜ ìˆëŠ” ì •ë³´ê°€ ì—†ìœ¼ë©´ "ì†Œì„¤ ë‚´ìš©ì— ê·¸ëŸ° ì •ë³´ëŠ” ë‚˜ì˜¤ì§€ ì•ŠìŠµë‹ˆë‹¤" ë˜ëŠ” "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤"ë¼ê³  ì†”ì§í•˜ê²Œ ë§í•˜ì„¸ìš”.
    4. ì»¨í…ìŠ¤íŠ¸ ë°–ì˜ ì¼ë°˜ ì§€ì‹ì´ë‚˜ ì¶”ì¸¡ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
    5. ì†Œì„¤ì— ë‚˜ì˜¤ëŠ” ì¸ë¬¼, ì¥ì†Œ, ì‚¬ê±´ì˜ ì´ë¦„ê³¼ í‘œí˜„ì„ ì •í™•íˆ ì‚¬ìš©í•˜ì„¸ìš”.
    
    [Context]:
    {{context}}
    """
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", template),
        ("human", "{question}")
    ])
    
    llm = ChatOpenAI(model="gpt-4o", temperature=0.7, openai_api_key=OPENAI_API_KEY)
    
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)
    
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain, retriever


def run_evaluation(
    session_id: str,
    dataset_path: str,
    character_name: str = "ìºë¦­í„°",
    k: int = 6  # ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜ (ê¸°ë³¸ê°’ ì¦ê°€: 4 -> 6)
):
    """
    RAGASë¥¼ ì‚¬ìš©í•œ í‰ê°€ ì‹¤í–‰
    """
    print(f"ğŸ“Š RAGAS í‰ê°€ ì‹œì‘...")
    print(f"ì„¸ì…˜ ID: {session_id}")
    print(f"ë°ì´í„°ì…‹: {dataset_path}")
    print("-" * 50)
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    eval_data = load_evaluation_dataset(dataset_path)
    print(f"âœ… í‰ê°€ ë°ì´í„° {len(eval_data)}ê°œ ë¡œë“œ ì™„ë£Œ")
    
    # RAG ì²´ì¸ ìƒì„±
    rag_chain, retriever = create_rag_chain(session_id, character_name, k=k)
    print("âœ… RAG ì²´ì¸ ìƒì„± ì™„ë£Œ")
    
    # ê° ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€ ìƒì„± ë° ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
    questions = []
    answers = []
    contexts_list = []
    ground_truths_list = []
    
    print("\nğŸ”„ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘...")
    for i, item in enumerate(eval_data):
        question = item["question"]
        questions.append(question)
        
        # Ground truth ì²˜ë¦¬
        if "ground_truths" in item:
            ground_truths_list.append(item["ground_truths"])
        elif "ground_truth" in item:
            ground_truths_list.append([item["ground_truth"]])
        else:
            ground_truths_list.append([])  # Ground truthê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸
        
        # RAG ì²´ì¸ìœ¼ë¡œ ë‹µë³€ ìƒì„±
        print(f"  [{i+1}/{len(eval_data)}] ì§ˆë¬¸: {question[:50]}...")
        answer = rag_chain.invoke(question)
        answers.append(answer)
        
        # ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ (LangChain ìµœì‹  ë²„ì „ì€ invoke ì‚¬ìš©)
        docs = retriever.invoke(question)
        contexts = [doc.page_content for doc in docs]
        contexts_list.append(contexts)
    
    print("\nâœ… ëª¨ë“  ì§ˆë¬¸ ì²˜ë¦¬ ì™„ë£Œ")
    print("\nğŸ“ˆ RAGAS ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
    
    # reference ì»¬ëŸ¼ ìƒì„± (ground_truthsì˜ ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©)
    # context_precisionì€ referenceê°€ í•„ìš”í•¨
    references = []
    for gt_list in ground_truths_list:
        if gt_list and len(gt_list) > 0:
            references.append(gt_list[0])  # ì²« ë²ˆì§¸ ground truthë¥¼ referenceë¡œ ì‚¬ìš©
        else:
            references.append("")  # ë¹ˆ ë¬¸ìì—´
    
    # RAGAS í‰ê°€ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹ ìƒì„±
    evaluation_dataset = Dataset.from_dict({
        "question": questions,
        "answer": answers,
        "contexts": contexts_list,
        "ground_truths": ground_truths_list,
        "reference": references,  # context_precisionì„ ìœ„í•´ ì¶”ê°€
    })
    
    # RAGAS í‰ê°€ ì‹¤í–‰ (embeddings ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬)
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    
    result = evaluate(
        dataset=evaluation_dataset,
        metrics=[
            faithfulness,
            answer_relevancy,  # embeddings í•„ìš”
            context_precision,
            context_recall,
        ],
        embeddings=embeddings,  # RAGASì— embeddings ì „ë‹¬
    )
    
    # ê²°ê³¼ ì¶œë ¥ (EvaluationResult ê°ì²´ë¡œ ì ‘ê·¼)
    print("\n" + "=" * 50)
    print("ğŸ“Š RAGAS í‰ê°€ ê²°ê³¼")
    print("=" * 50)
    print(f"\ní‰ê°€ëœ ì§ˆë¬¸ ìˆ˜: {len(questions)}")
    print(f"\në©”íŠ¸ë¦­ ì ìˆ˜:")
    
    # EvaluationResult ê°ì²´ì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°
    def get_metric_value(metric_name):
        try:
            # ì†ì„±ìœ¼ë¡œ ì ‘ê·¼ ì‹œë„
            value = getattr(result, metric_name, None)
            if value is None:
                # ë”•ì…”ë„ˆë¦¬ì²˜ëŸ¼ ì ‘ê·¼ ì‹œë„
                value = result[metric_name] if hasattr(result, '__getitem__') else None
            if value is None:
                return 0.0
            # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° í‰ê· ê°’ ì‚¬ìš©
            if isinstance(value, list):
                return sum(value) / len(value) if value else 0.0
            # ìˆ«ìì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
            return float(value)
        except (TypeError, ValueError, AttributeError, KeyError):
            return 0.0
    
    faithfulness_score = get_metric_value('faithfulness')
    answer_relevancy_score = get_metric_value('answer_relevancy')
    context_precision_score = get_metric_value('context_precision')
    context_recall_score = get_metric_value('context_recall')
    
    print(f"  - Faithfulness (ì •í™•ì„±): {faithfulness_score:.4f}")
    print(f"  - Answer Relevancy (ë‹µë³€ ê´€ë ¨ì„±): {answer_relevancy_score:.4f}")
    print(f"  - Context Precision (ì»¨í…ìŠ¤íŠ¸ ì •ë°€ë„): {context_precision_score:.4f}")
    print(f"  - Context Recall (ì»¨í…ìŠ¤íŠ¸ ì¬í˜„ìœ¨): {context_recall_score:.4f}")
    
    # ì „ì²´ í‰ê·  ê³„ì‚°
    avg_score = (faithfulness_score + answer_relevancy_score + context_precision_score + context_recall_score) / 4
    print(f"\n  ğŸ“Œ ì „ì²´ í‰ê·  ì ìˆ˜: {avg_score:.4f}")
    
    # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    output_file = f"evaluation_results_{session_id}.json"
    results_dict = {
        "session_id": session_id,
        "num_questions": len(questions),
        "metrics": {
            "faithfulness": faithfulness_score,
            "answer_relevancy": answer_relevancy_score,
            "context_precision": context_precision_score,
            "context_recall": context_recall_score,
            "average": avg_score,
        },
        "detailed_results": result.to_pandas().to_dict("records") if hasattr(result, 'to_pandas') else [],
    }
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results_dict, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ê°€ '{output_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return result


def create_sample_dataset(output_path: str = "evaluation_dataset.json"):
    """
    ìƒ˜í”Œ í‰ê°€ ë°ì´í„°ì…‹ ìƒì„± (ì˜ˆì‹œ)
    """
    sample_data = [
        {
            "question": "ìºë¦­í„°ì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ground_truth": "ìºë¦­í„°ì˜ ì´ë¦„ì€ [ì†Œì„¤ì—ì„œ ë‚˜ì˜¨ ì´ë¦„]ì…ë‹ˆë‹¤.",
        },
        {
            "question": "ìºë¦­í„°ì˜ ì„±ê²©ì€ ì–´ë–¤ê°€ìš”?",
            "ground_truth": "ìºë¦­í„°ëŠ” [ì†Œì„¤ì—ì„œ ë¬˜ì‚¬ëœ ì„±ê²©]ì…ë‹ˆë‹¤.",
        },
        {
            "question": "ì£¼ìš” ì‚¬ê±´ì€ ë¬´ì—‡ì´ì—ˆë‚˜ìš”?",
            "ground_truth": "ì£¼ìš” ì‚¬ê±´ì€ [ì†Œì„¤ì˜ ì£¼ìš” ì‚¬ê±´]ì…ë‹ˆë‹¤.",
        },
    ]
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(sample_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ìƒ˜í”Œ ë°ì´í„°ì…‹ì´ '{output_path}'ì— ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("   ì‹¤ì œ í‰ê°€ë¥¼ ìœ„í•´ ì§ˆë¬¸ê³¼ ì •ë‹µì„ ìˆ˜ì •í•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="RAGASë¥¼ ì‚¬ìš©í•œ RAG ì‹œìŠ¤í…œ í‰ê°€")
    parser.add_argument(
        "--session_id",
        type=str,
        required=False,
        help="í‰ê°€í•  ì„¸ì…˜ ID (--create_sample ì‚¬ìš© ì‹œ ë¶ˆí•„ìš”)"
    )
    parser.add_argument(
        "--dataset",
        type=str,
        default="evaluation_dataset.json",
        help="í‰ê°€ ë°ì´í„°ì…‹ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: evaluation_dataset.json)"
    )
    parser.add_argument(
        "--character_name",
        type=str,
        default="ìºë¦­í„°",
        help="ìºë¦­í„° ì´ë¦„ (ê¸°ë³¸ê°’: ìºë¦­í„°)"
    )
    parser.add_argument(
        "--k",
        type=int,
        default=6,
        help="ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜ (ê¸°ë³¸ê°’: 6)"
    )
    parser.add_argument(
        "--create_sample",
        action="store_true",
        help="ìƒ˜í”Œ í‰ê°€ ë°ì´í„°ì…‹ ìƒì„±"
    )
    
    args = parser.parse_args()
    
    if args.create_sample:
        create_sample_dataset(args.dataset)
    else:
        if not args.session_id:
            print("âŒ ì˜¤ë¥˜: --session_idê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            print("   ìƒ˜í”Œ ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ë ¤ë©´: python evaluate_ragas.py --create_sample")
            exit(1)
        
        if not os.path.exists(args.dataset):
            print(f"âŒ ì˜¤ë¥˜: ë°ì´í„°ì…‹ íŒŒì¼ '{args.dataset}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(f"   ìƒ˜í”Œ ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ë ¤ë©´ --create_sample ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            exit(1)
        
        run_evaluation(
            session_id=args.session_id,
            dataset_path=args.dataset,
            character_name=args.character_name,
            k=args.k
        )

